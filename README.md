Neural network written from scratch.\
\
It is based on a multilayer Rumelhart perceptron with a backpropagation algorithm aimed at supervised learning. All layers, with the exception of the input layer, have a nonlinear activation function. Efficiency is characterized by a quadratic cost estimate.\
Activation functions and cost computation are customizable.\
Includes multiple gradient descent algorithms: SGD, RMSProp\
\
Research paper for institute project based on this network.\
https://docs.google.com/document/d/1l5Cwcj6Or31qlnp08CF_jsxZd1QRCxho2snNXF9hFBE/edit?usp=sharing \
The study examines the dependence of the learning rate on the size of the training batch for several gradient descent methods.
